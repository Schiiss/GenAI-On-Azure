{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chunking Optimizations üìÉ\n",
    "\n",
    "In our case, the recursive text splitter will suffice for our chunking strategy. This text splitter is the recommended one for generic text. It is parameterized by a list of characters. It tries to split on them in order until the chunks are small enough. The default list is [\"\\n\\n\", \"\\n\", \" \", \"\"]. This has the effect of trying to keep all paragraphs (and then sentences, and then words) together as long as possible, as those would generically seem to be the strongest semantically related pieces of text.\n",
    "\n",
    "You can read more about the recursive text splitter [here](https://python.langchain.com/v0.2/docs/how_to/recursive_text_splitter/) in the LangChain docs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries üßë‚Äçüíª"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores.azuresearch import AzureSearch\n",
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to existing Azure OpenAI ü§ñ & Azure Search Instances üîé"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings: AzureOpenAIEmbeddings = AzureOpenAIEmbeddings(\n",
    "    azure_deployment=\"embeddings\",\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    ")\n",
    "\n",
    "index_name: str = \"products-optimized\"\n",
    "vector_store: AzureSearch = AzureSearch(\n",
    "    azure_search_endpoint=os.getenv(\"AZURE_SEARCH_ENDPOINT\"),\n",
    "    azure_search_key=os.getenv(\"AZURE_SEARCH_KEY\"),\n",
    "    index_name=index_name,\n",
    "    embedding_function=embeddings.embed_query,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunk, Vectorize, and Upsert our product documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in os.listdir('..\\sample-docs'):\n",
    "    if filename.endswith(\".txt\"):  # Adjust the file extension as needed\n",
    "        file_path = os.path.join('..\\sample-docs', filename)\n",
    "        \n",
    "        # Load the document\n",
    "        loader = TextLoader(file_path, encoding=\"utf-8\")\n",
    "        document = loader.load()\n",
    "        \n",
    "        # Split the document\n",
    "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=300)\n",
    "        docs = text_splitter.split_documents(document)\n",
    "        vector_store.add_documents(documents=docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question #1 ‚ùì"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = vector_store.similarity_search(\n",
    "    query=\"what smart phones do you sell?\",\n",
    "    k=3,\n",
    "    search_type=\"similarity\",\n",
    ")\n",
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question #2 ‚ùì"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = vector_store.similarity_search(\n",
    "    query=\"how much is the NexTech phone?\",\n",
    "    k=3,\n",
    "    search_type=\"similarity\",\n",
    ")\n",
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question #3 ‚ùì"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = vector_store.similarity_search(\n",
    "    query=\"what laptops do you have?\",\n",
    "    k=3,\n",
    "    search_type=\"similarity\",\n",
    ")\n",
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question #4 ‚ùì\n",
    "\n",
    "If you recall back to the previous module when we created our intial RAG application, the bot could not answer this question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = vector_store.similarity_search(\n",
    "    query=\"How much is the home theater system?\",\n",
    "    k=3,\n",
    "    search_type=\"similarity\",\n",
    ")\n",
    "print(docs[0].page_content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
