{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Optimizations - Final üèÅ\n",
    "\n",
    "Let's combine all the optimizations we learnt about in the previous video and combine them into one solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries üßë‚Äçüíª\n",
    "\n",
    "There are libraries we are all familiar with. They all come from previous videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "from langchain_community.vectorstores.azuresearch import AzureSearch\n",
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from semantic_router.layer import RouteLayer, Route\n",
    "from semantic_router.encoders import AzureOpenAIEncoder\n",
    "import logging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Establish Connection to GPT4o, Embeddings, and Azure Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AzureChatOpenAI(\n",
    "    azure_deployment=\"gpt4o\",\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "    api_version=\"2024-02-01\"\n",
    ")\n",
    "\n",
    "embeddings: AzureOpenAIEmbeddings = AzureOpenAIEmbeddings(\n",
    "    azure_deployment=\"embeddings\",\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    ")\n",
    "\n",
    "index_name: str = \"products-optimized\"\n",
    "vector_store: AzureSearch = AzureSearch(\n",
    "    azure_search_endpoint=os.getenv(\"AZURE_SEARCH_ENDPOINT\"),\n",
    "    azure_search_key=os.getenv(\"AZURE_SEARCH_KEY\"),\n",
    "    index_name=index_name,\n",
    "    embedding_function=embeddings.embed_query,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Logging Level \n",
    "\n",
    "This is set so we can see the querys the multiquery retriever generates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig()\n",
    "logging.getLogger(\"langchain.retrievers.multi_query\").setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Semantic Routes üõ£Ô∏è\n",
    "\n",
    "Let's define route for each of our products below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_talk = Route(\n",
    "    name=\"small_talk\",\n",
    "    utterances=[\n",
    "        \"Hey, how are you?\", \n",
    "        \"How's it going?\",\n",
    "        \"Nice weather today\"\n",
    "    ],\n",
    ")\n",
    "\n",
    "headphones_questions = Route(\n",
    "    name=\"headphones_questions\",\n",
    "    utterances=[\n",
    "        \"How much do the ClearSound X7 headphones cost?\",\n",
    "        \"What headphones do you offer?\",\n",
    "        \"What features does the SoundWave Elite 900 have?\"\n",
    "    ],\n",
    ")\n",
    "\n",
    "laptop_questions = Route(\n",
    "    name=\"laptop_questions\",\n",
    "    utterances=[\n",
    "        \"How much do the TechMax UltraBook 14 laptop cost?\",\n",
    "        \"What laptops do you offer?\",\n",
    "        \"What features does the SwiftBook Pro 13 have?\"\n",
    "    ],\n",
    ")\n",
    "\n",
    "smartphone_questions = Route(\n",
    "    name=\"smartphone_questions\",\n",
    "    utterances=[\n",
    "        \"How much do the TechMax NexTech Pro X smartphone cost?\",\n",
    "        \"What smartphones do you offer?\",\n",
    "        \"What features does the Galaxy Star G5 have?\"\n",
    "    ],\n",
    ")\n",
    "\n",
    "smartwatch_questions = Route(\n",
    "    name=\"smartwatch_questions\",\n",
    "    utterances=[\n",
    "        \"How much do the FitGear 6X smartwatch cost?\",\n",
    "        \"What smartwatchs do you offer?\",\n",
    "        \"What features does the ChronoTrack A1 have?\"\n",
    "    ],\n",
    ")\n",
    "\n",
    "home_theater_questions = Route(\n",
    "    name=\"home_theater_questions\",\n",
    "    utterances=[\n",
    "        \"How much does the Ultimate Home Theater System cost?\",\n",
    "        \"What home theater packages do you offer?\",\n",
    "        \"What features does the Ultimate Home Theater System have?\"\n",
    "    ],\n",
    ")\n",
    "\n",
    "routes = [small_talk, headphones_questions, laptop_questions, smartphone_questions, smartwatch_questions, home_theater_questions]\n",
    "encoder = AzureOpenAIEncoder(api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"), deployment_name=\"embeddings\", azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"), api_version=\"2024-02-15-preview\", model=\"text-embedding-ada-002\")\n",
    "rl = RouteLayer(encoder=encoder, routes=routes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create our Prompts ‚úçÔ∏è\n",
    "\n",
    "Each product will have it's own prompt and will be selected based on semantics from the customers input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "laptop_prompt = \"\"\"Your job is to answer questions on laptops\n",
    "Answer the question based only on the following context and be sure include citations (ie: laptop1 or laptop2):\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "smartphone_prompt = \"\"\"Your job is to answer questions on smartphones\n",
    "Answer the question based only on the following context and be sure include citations (ie: smartphone1 or smartphone2):\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "headphone_prompt = \"\"\"Your job is to answer questions on headphones\n",
    "Answer the question based only on the following context and be sure include citations (ie: headphones1 or headphones2):\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "smartwatch_prompt = \"\"\"Your job is to answer questions on smartwatches\n",
    "Answer the question based only on the following context and be sure include citations (ie: smartwatch1 or smartwatch2):\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "home_theater_prompt = \"\"\"Your job is to answer questions on the home theater package we sell\n",
    "Answer the question based only on the following context and be sure include citations (ie: hometheatersystem):\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "small_talk_prompt = ChatPromptTemplate.from_template(\"\"\"Your job is a friendly customer service bot. Respond to the user in a \n",
    "friendly way and remind them we have lots of tech products like headphones, smartwatches, laptops and more and ask how you can help\n",
    "\n",
    "Input: {input}\n",
    "\"\"\")\n",
    "\n",
    "chat_history_prompt = (\n",
    "    \"Given a chat history and the latest user question \"\n",
    "    \"which might reference context in the chat history, \"\n",
    "    \"formulate a standalone question which can be understood \"\n",
    "    \"without the chat history. Do NOT answer the question, \"\n",
    "    \"just reformulate it if needed and otherwise return it as is.\"\n",
    "    \"Original Question: {original_question}\"\n",
    "    \"Chat History: {chat_history}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create our Chains and MultiQueryRetriever ü¶ú\n",
    "\n",
    "Here we will define our semantic layer logic and when to trigger each chain. You will notice that each conditional contains a chain with the corresponding product prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever_from_llm = MultiQueryRetriever.from_llm(\n",
    "    retriever=vector_store.as_retriever(), llm=model\n",
    ")\n",
    "\n",
    "def semantic_layer(query: str):\n",
    "    route = rl(query)\n",
    "    result = None \n",
    "\n",
    "    if route.name == \"laptop_questions\":\n",
    "        prompt = ChatPromptTemplate.from_template(laptop_prompt)\n",
    "        retrieval_chain = (\n",
    "            {\"context\": retriever_from_llm, \"question\": RunnablePassthrough()}\n",
    "            | prompt\n",
    "            | model\n",
    "            | StrOutputParser()\n",
    "        )\n",
    "        result = retrieval_chain.invoke(query)\n",
    "    elif route.name == \"headphones_questions\":\n",
    "        prompt = ChatPromptTemplate.from_template(headphone_prompt)\n",
    "        retrieval_chain = (\n",
    "            {\"context\": retriever_from_llm, \"question\": RunnablePassthrough()}\n",
    "            | prompt\n",
    "            | model\n",
    "            | StrOutputParser()\n",
    "        )\n",
    "        result = retrieval_chain.invoke(query)\n",
    "    elif route.name == \"smartphone_questions\":\n",
    "        prompt = ChatPromptTemplate.from_template(smartphone_prompt)\n",
    "        retrieval_chain = (\n",
    "            {\"context\": retriever_from_llm, \"question\": RunnablePassthrough()}\n",
    "            | prompt\n",
    "            | model\n",
    "            | StrOutputParser()\n",
    "        )\n",
    "        result = retrieval_chain.invoke(query)\n",
    "    elif route.name == \"smartwatch_questions\":\n",
    "        prompt = ChatPromptTemplate.from_template(smartwatch_prompt)\n",
    "        retrieval_chain = (\n",
    "            {\"context\": retriever_from_llm, \"question\": RunnablePassthrough()}\n",
    "            | prompt\n",
    "            | model\n",
    "            | StrOutputParser()\n",
    "        )\n",
    "        result = retrieval_chain.invoke(query)\n",
    "    elif route.name == \"home_theater_questions\":\n",
    "        prompt = ChatPromptTemplate.from_template(home_theater_prompt)\n",
    "        retrieval_chain = (\n",
    "            {\"context\": retriever_from_llm, \"question\": RunnablePassthrough()}\n",
    "            | prompt\n",
    "            | model\n",
    "            | StrOutputParser()\n",
    "        )\n",
    "        result = retrieval_chain.invoke(query)\n",
    "    elif route.name == \"small_talk\":\n",
    "        chain = small_talk_prompt | model | StrOutputParser()\n",
    "        result = chain.invoke({\"input\": query})\n",
    "    else:\n",
    "        return(\"Sorry I cannot help you with that\")\n",
    "    \n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a ChatHistory class to store chat history üó®Ô∏è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatHistory:\n",
    "    def __init__(self):\n",
    "        self.queries = []\n",
    "\n",
    "    def add_query(self, query):\n",
    "        if len(self.queries) >= 10:\n",
    "            self.queries.pop(0)\n",
    "        self.queries.append(query)\n",
    "\n",
    "    def get_queries(self):\n",
    "        return self.queries\n",
    "    \n",
    "\n",
    "chat_history = ChatHistory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a function to process chat history\n",
    "\n",
    "This function takes in two arugments: \n",
    "\n",
    "1. The customers current question\n",
    "\n",
    "2. The customer questions\n",
    "\n",
    "The prompt will take both of those inputs and output as a standalone question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_chat_history(question, chat_history):\n",
    "    prompt = ChatPromptTemplate.from_template(\"Given a chat history and the latest user question \"\n",
    "        \"which might reference context in the chat history, \"\n",
    "        \"formulate a standalone question which can be understood \"\n",
    "        \"without the chat history. Do NOT answer the question, \"\n",
    "        \"just reformulate it if needed and otherwise return it as is.\"\n",
    "        \"Original Question: {question}\"\n",
    "        \"Chat History: {chat_history}\")\n",
    "    chain = prompt | model | StrOutputParser()\n",
    "    result = chain.invoke({\"question\": question, \"chat_history\": chat_history})\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Home Theater Question ‚ùì"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"what is the price of the home theater system?\"\n",
    "chat_history.add_query(query)\n",
    "result = process_chat_history(query, chat_history.get_queries())\n",
    "semantic_layer(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Home Theater Follow Up Question ‚ùì"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"what does it include?\"\n",
    "chat_history.add_query(query)\n",
    "result = process_chat_history(query, chat_history.get_queries())\n",
    "semantic_layer(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Laptops Question ‚ùì"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"what laptops do you have?\"\n",
    "chat_history.add_query(query)\n",
    "result = process_chat_history(query, chat_history.get_queries())\n",
    "semantic_layer(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Headphones Question ‚ùì"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"what is the cheapest pair of headphones?\"\n",
    "chat_history.add_query(query)\n",
    "result = process_chat_history(query, chat_history.get_queries())\n",
    "semantic_layer(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Small Talk üó£Ô∏è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"hello, how are you?\"\n",
    "chat_history.add_query(query)\n",
    "result = process_chat_history(query, chat_history.get_queries())\n",
    "semantic_layer(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
