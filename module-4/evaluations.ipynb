{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangSmith Evaluations üß™\n",
    "\n",
    "The pace of AI application development is often rate-limited by high-quality evaluations because there is a paradox of choice. Developers often wonder how to engineer their prompt or which LLM best balances accuracy, latency, and cost. High quality evaluations can help you rapidly answer these types of questions with confidence. LangSmith allows you to build high-quality [evaluations](https://docs.smith.langchain.com/concepts/evaluation) for your AI application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries üßë‚Äçüíª\n",
    "\n",
    "We are using a couple new libraries here, mostly, LangSmith libraries to take advantage of the evaluations the platform offers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "from langchain_community.vectorstores.azuresearch import AzureSearch\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "from langsmith.evaluation import LangChainStringEvaluator, evaluate\n",
    "from langsmith import Client, traceable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to existing Azure OpenAI ü§ñ & Azure Search Instances üîé"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AzureChatOpenAI(\n",
    "    azure_deployment=\"gpt4o\",\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "    api_version=\"2024-02-01\"\n",
    ")\n",
    "\n",
    "embeddings: AzureOpenAIEmbeddings = AzureOpenAIEmbeddings(\n",
    "    azure_deployment=\"embeddings\",\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    ")\n",
    "\n",
    "index_name: str = \"products-optimized\"\n",
    "vector_store: AzureSearch = AzureSearch(\n",
    "    azure_search_endpoint=os.getenv(\"AZURE_SEARCH_ENDPOINT\"),\n",
    "    azure_search_key=os.getenv(\"AZURE_SEARCH_KEY\"),\n",
    "    index_name=index_name,\n",
    "    embedding_function=embeddings.embed_query,\n",
    ")\n",
    "retriever_from_llm = MultiQueryRetriever.from_llm(\n",
    "    retriever=vector_store.as_retriever(), llm=model\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create RAG Bot Class ü§ñ\n",
    "\n",
    "Below we will mock our RAG application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RagBot:\n",
    "    def __init__(self, retriever, model: str = \"model\"):\n",
    "        self._retriever = retriever\n",
    "        # Initialize Azure OpenAI client\n",
    "        self._client = model = AzureChatOpenAI(azure_deployment=\"gpt4o\", azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"), api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"), api_version=\"2024-02-01\")\n",
    "        self._model = model\n",
    "\n",
    "    @traceable()\n",
    "    def retrieve_docs(self, question):\n",
    "        return self._retriever.invoke(question)\n",
    "\n",
    "    @traceable()\n",
    "    def invoke_llm(self, question, docs):\n",
    "        prompt = ChatPromptTemplate.from_template(\"\"\"Your job is to answer questions\n",
    "        Answer the question and include the context of what you used to answer.\n",
    "        {docs}\n",
    "        Question: {question}\"\"\")\n",
    "        chain = prompt | model | StrOutputParser()\n",
    "        result = chain.invoke({\"question\": question, \"docs\": docs})\n",
    "        return {\n",
    "            \"answer\": result,\n",
    "            \"contexts\": result,\n",
    "        }\n",
    "\n",
    "    @traceable()\n",
    "    def get_answer(self, question: str):\n",
    "        docs = self.retrieve_docs(question)\n",
    "        return self.invoke_llm(question, docs)\n",
    "\n",
    "# Replace `retriever_from_llm` with your actual retriever instance\n",
    "rag_bot = RagBot(retriever_from_llm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick Test üß™\n",
    "\n",
    "Is our retrieval working as expected?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = rag_bot.get_answer(\"what is the price of the home theater system?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create our dataset ‚ùìüó£Ô∏è\n",
    "\n",
    "Remember, datasets are collections of examples that provide the necessary inputs and expected reference outputs for assessing your AI application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [\n",
    "    \"what is the price of the home theater system?\",\n",
    "    \"Can you tell me about the customer reviews on the home theater system?\"\n",
    "]\n",
    "\n",
    "outputs = [\n",
    "    \"The price of the Ultimate Home Theater System (UHTS-1000) is $4999 (source: hometheatersystem.txt).\",\n",
    "    \"\"\"Certainly! The customer reviews for the Ultimate Home Theater System (UHTS-1000) are overwhelmingly positive. Here are some highlights:\n",
    "    \n",
    "        1. **John D.** praised the system for its picture quality and immersive sound:\n",
    "        - \"The picture on the 75-inch TV is crystal clear, and the colors are incredibly vibrant. The Dolby Atmos sound system takes my movie nights to the next level ‚Äì it‚Äôs like being in a theater! Highly recommend.\" ([hometheatersystem](#source)).\n",
    "\n",
    "        2. **Lisa M.** highlighted the display and sound quality:\n",
    "        - \"The OLED display provides perfect blacks and vibrant colors, and the Dolby Vision support makes everything look amazing. The soundbar and wireless subwoofer deliver incredible sound quality. Highly satisfied with my purchase.\" ([hometheatersystem](#source)).\n",
    "\n",
    "        3. **Sarah P.** appreciated the easy setup and flexibility of the wireless speakers:\n",
    "        - \"The setup was easy, and the wireless speakers give me the flexibility to arrange my living room however I like. The sound is immersive and powerful, and the TV‚Äôs smart features make it so easy to find and watch my favorite shows.\" ([hometheatersystem](#source)).\n",
    "\n",
    "        4. **Mark T.** was impressed by the 8K resolution and user-friendly interface:\n",
    "        - \"The 8K resolution is stunning, and the TV‚Äôs smart interface is very user-friendly. The surround sound system makes me feel like I‚Äôm in the middle of the action. Worth every penny!\" ([hometheatersystem](#source)).\n",
    "\n",
    "        Overall, customers seem to be highly satisfied with the UHTS-1000, praising its picture quality, sound performance, ease of setup, and smart features.\"\"\"\n",
    "]\n",
    "\n",
    "qa_pairs = [{\"question\": q, \"answer\": a} for q, a in zip(inputs, outputs)]\n",
    "client = Client()\n",
    "dataset_name = \"products_dataset\"\n",
    "dataset = client.create_dataset(\n",
    "    dataset_name=dataset_name,\n",
    "    description=\"QA Pairs for Generic Tech Shop Inc.\",\n",
    ")\n",
    "client.create_examples(\n",
    "    inputs=[{\"question\": q} for q in inputs],\n",
    "    outputs=[{\"answer\": a} for a in outputs],\n",
    "    dataset_id=dataset.id,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Evaluation üß™\n",
    "\n",
    "Let's run the evaluation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rag_answer(example: dict):\n",
    "    \"\"\"Use this for answer evaluation\"\"\"\n",
    "    response = rag_bot.get_answer(example[\"question\"])\n",
    "    return {\"answer\": response[\"answer\"]}\n",
    "\n",
    "qa_evalulator = [\n",
    "    LangChainStringEvaluator(\n",
    "        \"cot_qa\",\n",
    "        config={\n",
    "            \"llm\": model,\n",
    "        },\n",
    "        prepare_data=lambda run, example: {\n",
    "            \"prediction\": run.outputs[\"answer\"],\n",
    "            \"reference\": example.outputs[\"answer\"],\n",
    "            \"input\": example.inputs[\"question\"],\n",
    "        },\n",
    "    )\n",
    "]\n",
    "dataset_name = \"products_dataset\"\n",
    "experiment_results = evaluate(\n",
    "    get_rag_answer,\n",
    "    data=dataset_name,\n",
    "    evaluators=qa_evalulator,\n",
    "    experiment_prefix=\"rag-app\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
