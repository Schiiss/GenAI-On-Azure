{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "from langchain_community.vectorstores.azuresearch import AzureSearch\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "import logging\n",
    "model = AzureChatOpenAI(\n",
    "    azure_deployment=\"gpt4o\",\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "    api_version=\"2024-02-01\"\n",
    ")\n",
    "\n",
    "embeddings: AzureOpenAIEmbeddings = AzureOpenAIEmbeddings(\n",
    "    azure_deployment=\"embeddings\",\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    ")\n",
    "\n",
    "index_name: str = \"products-optimized\"\n",
    "vector_store: AzureSearch = AzureSearch(\n",
    "    azure_search_endpoint=os.getenv(\"AZURE_SEARCH_ENDPOINT\"),\n",
    "    azure_search_key=os.getenv(\"AZURE_SEARCH_KEY\"),\n",
    "    index_name=index_name,\n",
    "    embedding_function=embeddings.embed_query,\n",
    ")\n",
    "retriever_from_llm = MultiQueryRetriever.from_llm(\n",
    "    retriever=vector_store.as_retriever(), llm=model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import traceable\n",
    "\n",
    "class RagBot:\n",
    "    def __init__(self, retriever, model: str = \"model\"):\n",
    "        self._retriever = retriever\n",
    "        # Initialize Azure OpenAI client\n",
    "        self._client = model = AzureChatOpenAI(azure_deployment=\"gpt4o\", azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"), api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"), api_version=\"2024-02-01\")\n",
    "        self._model = model\n",
    "\n",
    "    @traceable()\n",
    "    def retrieve_docs(self, question):\n",
    "        return self._retriever.invoke(question)\n",
    "\n",
    "    @traceable()\n",
    "    def invoke_llm(self, question, docs):\n",
    "        prompt = ChatPromptTemplate.from_template(\"\"\"Your job is to answer questions\n",
    "        Answer the question and include the context of what you used to answer.\n",
    "        {docs}\n",
    "        Question: {question}\"\"\")\n",
    "        chain = prompt | model | StrOutputParser()\n",
    "        result = chain.invoke({\"question\": question, \"docs\": docs})\n",
    "        return {\n",
    "            \"answer\": result,\n",
    "            \"contexts\": result,\n",
    "        }\n",
    "\n",
    "    @traceable()\n",
    "    def get_answer(self, question: str):\n",
    "        docs = self.retrieve_docs(question)\n",
    "        return self.invoke_llm(question, docs)\n",
    "\n",
    "# Replace `retriever_from_llm` with your actual retriever instance\n",
    "rag_bot = RagBot(retriever_from_llm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = rag_bot.get_answer(\"what is the price of the home theater system?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import Client\n",
    "\n",
    "# QA\n",
    "inputs = [\n",
    "    \"what is the price of the home theater system?\",\n",
    "]\n",
    "\n",
    "outputs = [\n",
    "    \"The price of the Ultimate Home Theater System (UHTS-1000) is $4999 (source: hometheatersystem.txt).\",\n",
    "]\n",
    "\n",
    "qa_pairs = [{\"question\": q, \"answer\": a} for q, a in zip(inputs, outputs)]\n",
    "\n",
    "# Create dataset\n",
    "client = Client()\n",
    "dataset_name = \"RAG_test_LCEL\"\n",
    "dataset = client.create_dataset(\n",
    "    dataset_name=dataset_name,\n",
    "    description=\"QA pairs about LCEL.\",\n",
    ")\n",
    "client.create_examples(\n",
    "    inputs=[{\"question\": q} for q in inputs],\n",
    "    outputs=[{\"answer\": a} for a in outputs],\n",
    "    dataset_id=dataset.id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG chain\n",
    "def predict_rag_answer(example: dict):\n",
    "    \"\"\"Use this for answer evaluation\"\"\"\n",
    "    response = rag_bot.get_answer(example[\"question\"])\n",
    "    return {\"answer\": response[\"answer\"]}\n",
    "\n",
    "def predict_rag_answer_with_context(example: dict):\n",
    "    \"\"\"Use this for evaluation of retrieved documents and hallucinations\"\"\"\n",
    "    response = rag_bot.get_answer(example[\"question\"])\n",
    "    return {\"answer\": response[\"answer\"], \"contexts\": response[\"contexts\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith.evaluation import LangChainStringEvaluator, evaluate\n",
    "\n",
    "# Evaluator\n",
    "qa_evalulator = [\n",
    "    LangChainStringEvaluator(\n",
    "        \"cot_qa\",\n",
    "        config={\n",
    "            \"llm\": model,\n",
    "        },\n",
    "        prepare_data=lambda run, example: {\n",
    "            \"prediction\": run.outputs[\"answer\"],\n",
    "            \"reference\": example.outputs[\"answer\"],\n",
    "            \"input\": example.inputs[\"question\"],\n",
    "        },\n",
    "    )\n",
    "]\n",
    "dataset_name = \"RAG_test_LCEL\"\n",
    "experiment_results = evaluate(\n",
    "    predict_rag_answer,\n",
    "    data=dataset_name,\n",
    "    evaluators=qa_evalulator,\n",
    "    experiment_prefix=\"rag-qa-oai\",\n",
    "    metadata={\"variant\": \"LCEL context, gpt-3.5-turbo\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
