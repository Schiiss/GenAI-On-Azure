{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChain Overview ü¶ú\n",
    "\n",
    "<img src=\"../assets/cyborg_parrot.jpg\" alt=\"My Image\" style=\"width:200px;\">\n",
    "\n",
    "A framework for developing applications powered by large language models (LLMs)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Packages in LangChain\n",
    "\n",
    "- langchain-core: Package contains base abstractions that the rest of the LangChain ecosystem uses and is installed automatically when installing LangChain\n",
    "\n",
    "- langchain-community: Package contains third-party integrations\n",
    "\n",
    "- langchain-experimental: Package holds experimental LangChain code\n",
    "\n",
    "The packages we will be using are explicitly called out in our requirements.txt\n",
    "\n",
    "## What Value Does it Provide?\n",
    "\n",
    "- Provides abstractions for integrating with a large variety of LLM's\n",
    "\n",
    "- Provides mechanisms to implement complex patterns like chat history, agents etc.\n",
    "\n",
    "- Offers libraries to easily prep documents for Generative AI\n",
    "\n",
    "- Provides the 'LangChain Expression Language' which enables developers to quickly build LLM workflows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_community.chat_message_histories import SQLChatMessageHistory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LangChain Expression Language üë®‚Äçüíª\n",
    "\n",
    "Below, we will leverage the LangChain Expression Language to do the following:\n",
    "\n",
    "- Create a [prompt template](https://python.langchain.com/v0.2/docs/how_to/#prompt-templates) which will help format user input into a format that can be passed to a language model\n",
    "\n",
    "- Leverage the format output from the prompt template to pass along to a [chat model](https://python.langchain.com/v0.2/docs/how_to/#chat-models). In our case, the chat model is GPT4o\n",
    "\n",
    "- Convert the output into a string leveraging one of LangChains [output parsers](https://python.langchain.com/v0.2/docs/how_to/#output-parsers) called StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AzureChatOpenAI(\n",
    "    azure_deployment=\"gpt4o\",\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "    api_version=\"2024-02-01\"\n",
    ")\n",
    "prompt = ChatPromptTemplate.from_template(\"summarize the following Star Wars Movie:{movie}\")\n",
    "chain = prompt | model | StrOutputParser()\n",
    "chain.invoke({\"movie\": \"A New Hope\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making LLM Call's Stateful (ie: chat history) üí¨\n",
    "\n",
    "As mentioned above, one of the nice things about LangChain is the abstractions it provides to solve common technical challenges when building LLM enabled applications, chat history being a common one.\n",
    "\n",
    "Let's start by defining where we will store out chat history. In our case, we will leverage a local sqlite database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_session_history(session_id):\n",
    "    return SQLChatMessageHistory(session_id, \"sqlite:///memory.db\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's boiler plate a prompt template that accepts the users message and preferred language as input. We will leverage the sqlite database to save and append the chat history to the prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You're an assistant who speaks in {language}. Respond in 20 words or fewer\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "runnable = prompt | model\n",
    "\n",
    "runnable_with_history = RunnableWithMessageHistory(\n",
    "    runnable,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"history\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's send an initial message introducing ourselves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runnable_with_history.invoke(\n",
    "    {\"language\": \"english\", \"input\": \"hi im Conner!\"},\n",
    "    config={\"configurable\": {\"session_id\": \"1\"}},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, lets follow up and ask what our name is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runnable_with_history.invoke(\n",
    "    {\"language\": \"english\", \"input\": \"whats my name?\"},\n",
    "    config={\"configurable\": {\"session_id\": \"1\"}},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Prep üìÑ\n",
    "\n",
    "- Prepping documents for RAG is a critical step \n",
    "\n",
    "- LangChain exposes many interfaces to [load documents](https://python.langchain.com/v0.2/docs/integrations/document_loaders/)\n",
    "\n",
    "- LangChain can help with loading, chunking, and upserting into a vector store"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
