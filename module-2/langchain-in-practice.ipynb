{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChain in Practice ü¶ú\n",
    "\n",
    "A framework for developing applications powered by large language models (LLMs)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Packages in LangChain\n",
    "\n",
    "- langchain-core: Package contains base abstractions that the rest of the LangChain ecosystem uses and is installed automatically when installing LangChain\n",
    "\n",
    "- langchain-community: Package contains third-party integrations\n",
    "\n",
    "- langchain-experimental: Package holds experimental LangChain code\n",
    "\n",
    "The packages we will be using are explicitly called out in our requirements.txt\n",
    "\n",
    "## What Value Does it Provide?\n",
    "\n",
    "- Provides abstractions for integrating with a large variety of LLM's\n",
    "\n",
    "- Offers libraries to easily prep documents for Generative AI\n",
    "\n",
    "- Provides the 'LangChain Expression Language' which enables developers to quickly build LLM workflows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from typing import Any, Dict, List\n",
    "load_dotenv()\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_community.chat_message_histories import SQLChatMessageHistory\n",
    "from langchain_core.callbacks import BaseCallbackHandler\n",
    "from langchain_core.outputs import LLMResult\n",
    "from langchain_core.messages import BaseMessage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define our GPT4o Model ü§ñ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AzureChatOpenAI(\n",
    "    azure_deployment=\"gpt4o\",\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "    api_version=\"2024-02-01\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create our Callback Handler Class üìû"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoggingHandler(BaseCallbackHandler):\n",
    "    def on_chat_model_start(\n",
    "        self, serialized: Dict[str, Any], messages: List[List[BaseMessage]], **kwargs\n",
    "    ) -> None:\n",
    "        print(\"Chat model started\")\n",
    "\n",
    "    def on_llm_end(self, response: LLMResult, **kwargs) -> None:\n",
    "        print(f\"Chat model ended, response: {response}\")\n",
    "\n",
    "    def on_chain_start(\n",
    "        self, serialized: Dict[str, Any], inputs: Dict[str, Any], **kwargs\n",
    "    ) -> None:\n",
    "        print(f\"Chain {serialized.get('name')} started\")\n",
    "\n",
    "    def on_chain_end(self, outputs: Dict[str, Any], **kwargs) -> None:\n",
    "        print(f\"Chain ended, outputs: {outputs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LangChain Expression Language (chains) üë®‚Äçüíª\n",
    "\n",
    "Below, we will leverage the LangChain Expression Language to do the following:\n",
    "\n",
    "- Create a [prompt template](https://python.langchain.com/v0.2/docs/how_to/#prompt-templates) which will help format user input into a format that can be passed to a language model\n",
    "\n",
    "- Leverage the format output from the prompt template to pass along to a [chat model](https://python.langchain.com/v0.2/docs/how_to/#chat-models). In our case, the chat model is GPT4o\n",
    "\n",
    "- Convert the output into a string leveraging one of LangChains [output parsers](https://python.langchain.com/v0.2/docs/how_to/#output-parsers) called StrOutputParser\n",
    "\n",
    "- Enable [Callbacks](https://python.langchain.com/v0.2/docs/how_to/callbacks_runtime/) so we can monitor the LLM call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [LoggingHandler()]\n",
    "prompt = ChatPromptTemplate.from_template(\"summarize the following Star Wars Movie:{movie}\")\n",
    "chain = prompt | model | StrOutputParser()\n",
    "chain.invoke({\"movie\": \"A New Hope\"}, config={\"callbacks\": callbacks})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making LLM Call's Stateful (ie: chat history) üí¨\n",
    "\n",
    "As mentioned above, one of the nice things about LangChain is the abstractions it provides to solve common technical challenges when building LLM enabled applications, chat history being a common one.\n",
    "\n",
    "Let's start by defining where we will store out chat history. In our case, we will leverage a local sqlite database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_session_history(session_id):\n",
    "    return SQLChatMessageHistory(session_id, \"sqlite:///memory.db\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's boiler plate a prompt template that accepts the users message and preferred language as input. We will leverage the sqlite database to save and append the chat history to the prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You're an assistant who speaks in {language}. Respond in 20 words or fewer\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "runnable = prompt | model\n",
    "\n",
    "runnable_with_history = RunnableWithMessageHistory(\n",
    "    runnable,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"history\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's send an initial message introducing ourselves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runnable_with_history.invoke(\n",
    "    {\"language\": \"english\", \"input\": \"hi im Conner!\"},\n",
    "    config={\"configurable\": {\"session_id\": \"1\"}},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, lets follow up and ask what our name is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runnable_with_history.invoke(\n",
    "    {\"language\": \"english\", \"input\": \"whats my name?\"},\n",
    "    config={\"configurable\": {\"session_id\": \"1\"}},\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
